# 활성화 함수 (Activation Functions)

## 개요

활성화 함수는 인공 신경망에서 뉴런의 출력값을 결정하는 함수로, **비선형성**을 도입하여 모델이 복잡한 패턴을 학습할 수 있도록 합니다

다양한 활성화 함수가 존재하며, 각 함수는 고유한 특성과 장단점을 가지고 있습니다

적절한 활성화 함수 선택은 학습 속도, 수렴 안정성, 성능에 직접적인 영향을 미칩니다

## 시그모이드 함수 (Sigmoid Function)

- **정의**: 시그모이드 함수는 입력값을 0과 1 사이로 압축하는 S자 형태의 함수입니다

  $\sigma(x) = \frac{1}{1 + e^{-x}}$

<img src="https://i.ibb.co/d4gFrq7g/image.png" alt="image" border="0">

- **특징**
  - 출력 범위: (0, 1)
  - 미분 가능
  - 출력이 0 또는 1에 가까워지면 기울기 소실(vanishing gradient) 문제가 발생할 수 있음

- **장점**
  - 출력이 확률로 해석 가능 → 이진 분류 문제에서 출력층으로 적합

- **단점**
  - 기울기 소실 문제로 깊은 신경망 학습이 어려움
  - 출력값이 0 근처가 아닌 양쪽 끝에서만 변화 → 학습 속도 느림

- **용도**
  - 출력층에서 이진 분류 문제 확률 출력용
  - 작은 네트워크나 간단한 문제에서 사용 가능

## 하이퍼볼릭 탄젠트 함수 (Tanh Function)

- **정의**: 하이퍼볼릭 탄젠트 함수는 시그모이드 함수의 확장으로, 입력값을 -1과 1 사이로 압축하는 함수입니다

  $\tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$

<img src="https://i.ibb.co/dJ7XGdNg/image.png" alt="image" border="0">

- **특징**
  - 출력 범위: (-1, 1)
  - 미분 가능
  - 시그모이드 함수보다 출력값의 평균이 0에 가까워 학습이 더 빠를 수 있음

- **장점**
  - 출력 중심이 0 → 데이터 정규화 효과
  - 은닉층에서 시그모이드보다 학습 효율이 좋음

- **단점**
  - 여전히 입력이 큰 값에서는 기울기 소실
  - 깊은 네트워크에서는 ReLU 계열 대비 학습 느림

- **용도**
  - 은닉층 활성화 함수
  - 시그모이드보다 출력 중심이 0인 문제에서 적합

## ReLU

- **정의**: ReLU 함수는 입력값이 0보다 크면 그대로 출력하고, 0 이하일 경우 0을 출력하는 함수입니다

  $\text{ReLU}(x) = \max(0, x)$

<img src="https://i.ibb.co/gbmXzxk3/image.png" alt="image" border="0">

- **특징**
  - 출력 범위: [0, ∞)
  - 계산이 간단하고 빠름
  - 음수 입력에 대해 기울기가 0이 되어 뉴런이 죽는 현상(Dead Neuron) 발생 가능

- **장점**
  - 계산 효율 높음 → 대규모 신경망 학습 적합
  - 희소 활성화(sparse activation) 가능 → 메모리 효율적

- **단점**
  - 음수 입력에서 뉴런이 죽어 업데이트 되지 않음
  - 초기화와 학습률 선택에 민감

- **용도**
  - 은닉층에서 기본 선택지
  - CNN, 딥러닝 모델에서 널리 사용

## Leaky ReLU

- **정의**: Leaky ReLU는 ReLU의 변형으로, 음수 입력에 대해 작은 기울기를 허용하여 뉴런이 죽는 현상을 완화합니다

  $\text{Leaky ReLU}(x) = \begin{cases} 
  x & \text{if } x > 0 \\
  \alpha x & \text{if } x \leq 0
  \end{cases}$

  여기서 $\alpha$는 작은 상수입니다

<img src="https://i.ibb.co/CKZHJSdB/image.png" alt="image" border="0">

- **특징**
  - 출력 범위: (-∞, ∞)
  - 음수 입력에 대해 작은 기울기를 허용하여 학습이 지속됨
  - $\alpha$ 값에 따라 성능이 달라질 수 있음

- **장점**
  - ReLU보다 안정적인 학습 가능
  - 뉴런 죽음 문제 감소

- **단점**
  - $\alpha$ 값 선택에 따라 성능 변동 가능
  - ReLU보다 계산 비용 약간 증가

- **용도**
  - ReLU의 단점을 보완하고 싶은 경우 은닉층에서 사용

## ELU

- **정의**: ELU 함수는 음수 입력에 대해 지수 함수를 적용하여 출력하는 활성화 함수입니다

  $\text{ELU}(x) = \begin{cases} 
  x & \text{if } x > 0 \\
  \alpha (e^{x} - 1) & \text{if } x \leq 0
  \end{cases}$

  여기서 $\alpha$는 하이퍼파라미터입니다

<img src="https://i.ibb.co/Kxz5v9KN/image.png" alt="image" border="0">

- **특징**
  - 출력 범위: (-$\alpha$, ∞)
  - 음수 입력에 대해 부드러운 출력을 제공
  - 기울기 소실 문제를 완화하고, 학습 속도를 향상시킬 수 있음

- **장점**
  - 음수 입력에서도 학습 지속
  - 평균 출력이 0에 가까워 학습 안정성 향상

- **단점**
  - 계산량 증가
  - 하이퍼파라미터 $\alpha$ 조정 필요

- **용도**
  - 은닉층에서 ReLU보다 안정적인 학습이 필요할 때
  - 깊은 신경망에서 성능 향상 목적

## 결론

활성화 함수는 **신경망 학습의 핵심 요소**입니다

- 출력층에서는 문제 특성에 맞게 Sigmoid, Softmax 등을 사용합니다
- 은닉층에서는 ReLU 계열이 기본이며, Dead Neuron이나 기울기 소실 문제를 완화하기 위해 Leaky ReLU, ELU 등을 활용할 수 있습니다 
- 최적의 활성화 함수는 **데이터 특성과 모델 구조에 따라 달라지므로 실험과 검증을 통한 선택이 필수**입니다

## 참고자료

- [활성화 함수에 대한 이해](https://medium.com/@hugmanskj/%ED%99%9C%EC%84%B1%ED%99%94-%ED%95%A8%EC%88%98%EC%97%90-%EB%8C%80%ED%95%9C-%EC%9D%B4%ED%95%B4-401b69ad443e)

<br>

> 작성 - `2025.09.16`<br>
> 마지막 수정 - `2025.09.16`
