# 고유값 분해(EVD)와 주성분 분석(PCA)

## 고유값과 고유벡터

정방행렬 $A \in \mathbb{R}^{n \times n}$에 대해, 벡터 $v \in \mathbb{R}^n$와 스칼라 $\lambda$가 다음을 만족하면:

$
A v = \lambda v
$

- $v$: 고유벡터 (eigenvector)  
- $\lambda$: 고유값 (eigenvalue)  

즉, 행렬 $A$가 벡터 $v$를 단순히 크기만 $\lambda$배 하여 변환하는 경우를 의미합니다

$
A V = V \Lambda
$

행렬 $A$가 $n$개의 선형 독립적인 고유벡터를 가지면, 다음과 같이 분해할 수 있습니다

$
A = V \Lambda V^{-1}
$

- $V$: 고유벡터들을 열(column)로 가지는 행렬  
- $\Lambda$: 고유값들을 대각선에 배치한 대각행렬  

즉, $A$는 고유벡터와 고유값을 통해 대각화가 가능합니다

<img src="https://i.ibb.co/HLdfN0N3/image.png" alt="image" border="0">

<img src="https://i.ibb.co/1chqn7Q/image.png" alt="image" border="0">

## 고유값 계산

고유값은 다음 **특성방정식(characteristic equation)**을 풀어 구합니다

$
\det(A - \lambda I) = 0
$

여기서 $I$는 단위행렬이고, 이 방정식의 해가 고유값 $\lambda$가 됩니다

<br>

## 주성분 분석 (PCA)

주성분 분석(PCA)은 고차원 데이터를 **저차원 공간으로 투영**하여 데이터를 압축하고, 가장 중요한 패턴을 찾아내는 차원 축소 기법입니다

데이터의 **분산(variance)**이 가장 큰 방향을 찾고, 그 방향을 새로운 좌표축(주성분)으로 합니다

### 직관적인 아이디어

- 데이터는 고차원 공간에 흩어져 있음
- 분산이 큰 방향일수록 데이터의 **정보(특징)**를 많이 담고 있음
- 따라서 데이터를 그 방향으로 정렬(투영)하면 **차원을 줄이면서도 손실을 최소화**할 수 있음

즉, PCA는 데이터를 설명하는 "가장 중요한 축"을 찾는 과정

### 수학적 정의

1. **데이터 평균 정규화 (Centering)**  
   - $
   X' = X - \mu
   $  
   - 각 변수에서 평균을 빼서 원점을 기준으로 정렬

2. **공분산 행렬 계산**  
   - $
   C = \frac{1}{n} (X')^T X'
   $  
   - 데이터 간의 분산과 상관관계를 나타내는 행렬

3. **고유값 분해 (Eigenvalue Decomposition)**  
   - $
   C v_i = \lambda_i v_i
   $  
   - $v_i$: 고유벡터 (주성분 방향)  
   - $\lambda_i$: 고유값 (해당 방향의 분산 크기)  

4. **주성분 선택**  
   - 고유값이 큰 순서대로 고유벡터를 선택
   - $
   Z = X' V_k
   $  
   - $V_k$: 상위 $k$개의 고유벡터 (주성분)  
   - $Z$: 차원이 축소된 데이터

### 활용 사례
- **데이터 압축**: 이미지 차원 축소 (픽셀 수 줄이기)  
- **노이즈 제거**: 작은 분산 방향은 잡음일 가능성이 높음  
- **시각화**: 고차원 데이터를 2D, 3D로 축소해 시각화  
- **머신러닝 전처리**: 불필요한 변수 제거, 학습 속도 향상

<br>

> 참고자료 - [공돌이의 수학정리노트 (Angelo's Math Notes)](https://angeloyeo.github.io/2020/11/19/eigen_decomposition.html)<br>
> 2025.09.10 작성