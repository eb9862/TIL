## 선형 변환 (Linear Transformation)

선형 변환은 수학적으로 **벡터 공간을 다른 벡터 공간으로 변환할 때 두 조건을 만족하는 연산**을 말합니다:

1. **덧셈에 대한 선형성 (Additivity)**
    
    $T(x+y)=T(x)+T(y)$
    
2. **스칼라 곱에 대한 선형성 (Homogeneity)**
    
    $T(cx)=cT(x)$
    

즉, 입력 벡터의 합과 스칼라 배율이 그대로 출력 벡터에 반영되는 변환입니다.

### 예시

- 행렬 곱: $\mathbf{y} = W \mathbf{x}$
- 2D 회전, 스케일링, 반사 같은 기하학적 변환

## 비선형 변환(Non-linear Transformation)

비선형 변환은 선형 조건을 만족하지 않는 변환입니다. 즉,

$T(x+y)≠T(x)+T(y)\, or \, T(cx)≠cT(x)$

### 예시

- 활성 함수(ReLU, Sigmoid, Tanh)
- 다항식 변환, 제곱, 지수 연산 등

### **딥러닝에서 중요한 이유**

- 선형 변환만 여러 층을 쌓으면 **여전히 선형 변환**이므로, 표현력이 제한됨
- 비선형 활성 함수를 사용해야 **복잡한 비선형 패턴**을 학습 가능

## 정리

| 구분 | 선형 변환 | 비선형 변환 |
| --- | --- | --- |
| 정의 | 입력의 합, 스칼라 배율에 선형 | 선형 조건 불만족 |
| 예시 | 행렬 곱, 벡터 회전, 스케일링 | ReLU, Sigmoid, 다항식, 지수 |
| 딥러닝 구현 | W·x + b (FC Layer, Conv Layer) | 활성 함수(ReLU, Sigmoid 등) |
| 역할 | 입력 벡터를 새로운 특징 공간으로 변환 | 모델의 표현력 증가, 복잡한 패턴 학습 가능 |


> 2025.09.08 작성