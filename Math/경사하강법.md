# 🛝 경사하강법 (Gradient Descent)

## 기본 개념

- 목적: 손실 함수 $L(θ)$를 최소화하는 $θ$ 찾기

- 아이디어: 기울기(gradient)를 따라 매개변수를 조금씩 이동

- 업데이트 식: $θ^{(t+1)}=θ^{(t)}−η∇_θ​L(θ^{(t)})$

  - $η$: 학습률(learning rate)
  - $∇_θ​L(θ_{(t)})$: 현재 위치에서의 기울기

<img src="https://hackernoon.com/hn-images/1*ZmzSnV6xluGa42wtU7KYVA.gif">

- 미분값은 주어진 점 $(x, f(x))$에서의 접선의 기울기를 의미합니다.

- 이 값을 빼면 경사하강법(gradient descent)이라 하며 함수의 극소값의 위치를 구할 때 사용 → 이를 통해 손실 함수를 최소화

## 경사하강법의 문제점

- 극값(최소값, 최대값)에 도달하면 더 이상 업데이트 불가

- 기울기 값이 0에 가까우면 수렴 속도가 매우 느려짐

- 딥러닝 손실 함수는 비볼록(non-convex) → 여러 지역 최소값(local minima) 존재 → 전역 최적점 도달이 어려움

<img src="https://i.ibb.co/zHQHCHNz/image.png" alt="image" border="0" height=400>

## 그래디언트 벡터

- 그래디언트 벡터는 모든 변수에 대한 편도함수들을 모아 놓은 벡터

- 함수의 가장 가파른 증가 방향을 나타냄.
  - 경사하강법은 그 반대 방향(-그래디언트)으로 이동하며 최소값을 찾음
- $∇f(x)=(∂_{x1}f​,∂_{x2}​f​,…,∂_{xd}​f​)$

- $x^2 + xy + y^2$의 그래디언트 벡터를 도식화

  <table>
    <img src="https://i.ibb.co/m5Vq8c1R/image.png" alt="image" border="0" width=50%>
    <img src="https://i.ibb.co/bgjXYgTK/image.png" alt="image" border="0" width=50%>
  </table>


## 확률적 경사하강법 (Stochastic Gradient Descent, SGD)

### 기본 아이디어

- 전체 데이터셋 $D=(X,y)$를 사용하는 대신 **데이터 일부(미니배치)**로 기울기를 계산

- 업데이트 식: $θ^{(t+1)}=θ^{(t)}−η∇_θ​L(D^{(b)}, θ^{(t)})$
  - $D(b)=(X(b),y(b))⊂D$ (무작위 샘플링된 미니배치)

### 장점

- 계산 효율 ↑
  - 전체 데이터 대신 일부 데이터만 사용 → 연산량 감소
- 큰 데이터셋에서도 빠르게 학습 가능
  - 일반적인 경사하강법처럼 모든 데이터를 업로드하면 메모리 부족 발생 가능
- 비볼록 함수 최적화에 적합️

### 단점

- 매 업데이트마다 미니배치가 달라 손실 함수 곡선이 요동(noisy)
  - 진동(oscillation)이 발생할 수 있음

- 수렴 안정성을 위해 학습률 스케줄링(learning rate decay) 필요
  - 학습 초반엔 큰 학슬률, 후반엔 작은 학습률이 되도록 학습률을 점진적으로 줄임

## 경사하강법 vs 확률적 경사하강법 비교
| 구분         | 경사하강법(GD) | 확률적 경사하강법(SGD) |
| ---------- | --------- | -------------- |
| 데이터 사용     | 전체 데이터    | 미니배치 (일부)      |
| 연산량        | 큼         | 작음             |
| 수렴 속도      | 안정적이나 느림  | 빠르지만 요동침       |
| 비볼록 함수 최적화 | 어려움       | 가능             |
| 실제 딥러닝     | 잘 쓰이지 않음  | 표준 방법          |

## SGD 학습 시 유의사항

#### 학습률 설정

- 너무 크면 발산(divergence)
- 너무 작으면 수렴 속도 저하

#### 에포크(epoch)

- 모든 데이터를 한 번 다 사용하면 1 epoch
- 여러 epoch을 통해 점진적으로 수렴

#### 배치 크기(batch size)

- 너무 작으면 요동 심함
- 너무 크면 계산 부담 ↑

## 요약

- 경사하강법은 전체 데이터를 사용해 안정적이나 비효율적

- 확률적 경사하강법(SGD)은 일부 데이터를 사용해 효율적이고, 딥러닝 최적화에서 실질적으로 더 나음

- 실제 딥러닝 학습의 기본은 SGD + 변형 알고리즘(Adam, RMSProp 등)

<br>

> 참고자료 - [공돌이의 수학정리노트 (Angelo's Math Notes)](https://angeloyeo.github.io/2020/08/16/gradient_descent.html)<br>
> 2025.09.09 작성
