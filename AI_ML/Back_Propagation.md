# 오차 역전파 (Backpropagation)

## 개요

오차 역전파(backpropagation)는 인공 신경망에서 가중치를 학습하기 위해 사용되는 효율적인 알고리즘으로, 주로 경사 하강법(gradient descent)과 함께 사용됩니다

이 알고리즘은 출력층에서부터 입력층으로 거꾸로 오차를 전파하여 각 가중치에 대한 손실 함수의 기울기를 계산합니다

이를 통해 네트워크의 가중치를 조정하여 예측 정확도를 향상시킬 수 있습니다

## 동작 원리

1. **순전파 (Forward Propagation)**: 입력 데이터를 네트워크에 통과시켜 예측값을 계산합니다

2. **오차 계산 (Error Calculation)**: 예측값과 실제값 간의 오차를 계산합니다. 일반적으로 손실 함수(예: MSE, 교차 엔트로피)를 사용하여 오차를 측정합니다

3. **역전파 (Backward Propagation)**: 출력층에서 입력층으로 거꾸로 오차를 전파하며, 각 가중치에 대한 손실 함수의 기울기를 계산합니다

4. **가중치 업데이트 (Weight Update)**: 계산된 기울기를 사용하여 가중치를 조정합니다. 일반적으로 경사 하강법을 사용하여 가중치를 업데이트합니다

## 수학적 표현

퍼셉트론이나 다층 신경망에서 오차 역전파는 **순전파 → 오차 계산 → 역전파 → 가중치 업데이트** 순으로 진행됩니다

### 순전파 (Forward Pass)

  각 층의 출력은 다음과 같이 계산됩니다
  
  $z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]}$
  
  $a^{[l]} = f(z^{[l]})$

  - $a^{[l-1]}$ : 이전 층의 출력 (입력층에서는 $x$)
  - $W^{[l]}$ : 현재 층의 가중치
  - $b^{[l]}$ : 현재 층의 편향
  - $f$ : 활성화 함수 (예: sigmoid, ReLU)
  
  즉, **입력값 × 가중치 + 편향 → 활성화 함수 적용 → 출력** 순서입니다

### 오차 계산

출력층에서의 오차 $\delta^{[L]}$는 손실 함수에 대한 출력값의 미분으로 계산합니다

$\delta^{[L]} = \frac{\partial \mathcal{L}}{\partial a^{[L]}} \odot f'(z^{[L]})$

- $\mathcal{L}$ : 손실 함수 (MSE, Cross-Entropy 등)
- $f'$ : 활성화 함수의 미분
- $\odot$ : 원소별 곱 (각 뉴런별로 미분 적용)

> > 참고: 출력층에서 Sigmoid 활성화와 Cross-Entropy 손실을 사용할 경우, $δ^[L] = a^[L] - y$ 로 간단히 계산할 수 있습니다.
> 이는 미분 과정에서 $f'(z^[L])$ 항이 손실 함수 미분과 상쇄되기 때문입니다.


<img src="https://i.ibb.co/KcB9B5CK/image.png" alt="image" border="0">


### 역전파 (Backward Pass)

$W^{[l]} \leftarrow W^{[l]} - \eta \cdot \delta^{[l]} (a^{[l-1]})^T$

$b^{[l]} \leftarrow b^{[l]} - \eta \cdot \delta^{[l]}$

- $\eta$ : 학습률
- $\delta^{[l]}$ : 현재 층의 오차
- $a^{[l-1]}$ : 이전 층 출력(입력)

### 한눈에 보는 흐름

1. 순전파: 입력 → 가중합 → 활성화 → 출력  
2. 출력 오차 계산: 실제값 - 예측값  
3. 역전파: 오차를 각 층으로 전파하며 기울기 계산  
4. 가중치 업데이트: 기울기에 학습률을 곱해 조정

> 참고: 출력층 오차 δ 계산은 손실 함수에 따라 달라집니다.
> - MSE 사용 시: $δ = y - a^[L]$
> - Cross-Entropy + Sigmoid 사용 시: $δ = a^[L] - y$

이렇게 표현하면 체인 룰(chain rule)을 기반으로 한 역전파 수식을 **출력에서 입력으로 오차를 전파**하는 과정으로 직관적으로 이해하기 쉬워집니다

## 장점

- **효율성**: 체인 룰(chain rule)을 활용하여 각 가중치에 대한 기울기를 효율적으로 계산할 수 있습니다

- **확장성**: 다층 신경망에서도 적용 가능하며, 복잡한 모델 학습에 유용합니다

- **일반화 가능성**: 다양한 손실 함수와 활성화 함수에 적용할 수 있습니다


## 한계점

- **기울기 소실/폭주 문제**: 깊은 신경망에서는 역전파 중에 기울기가 매우 작아지거나 커져서 학습이 어려워질 수 있습니다

- **국소 최솟값(local minima)**: 비선형 함수의 경우, 최적해가 아닌 국소 최솟값에 수렴할 수 있습니다

- **계산 비용**: 대규모 데이터셋과 복잡한 모델에서는 계산 비용이 크게 증가할 수 있습니다


## 개선 방법

- **가중치 초기화 기법**: He 초기화, Xavier 초기화 등을 사용하여 학습 초기에 기울기 소실/폭주 문제를 완화할 수 있습니다

- **배치 정규화(Batch Normalization)**: 각 층의 입력을 정규화하여 학습을 안정화시킬 수 있습니다

- **잔차 연결(Residual Connections)**: 깊은 신경망에서 정보 손실을 방지하고 기울기 소실 문제를 완화할 수 있습니다

## 참고자료

- [딥러닝 개론 및 연습 - 오차역전파](https://compmath.korea.ac.kr/deeplearning/BackPropagation.html)
- [오차역전파 (Backprogation)의 개념을 쉽게 이해해 봅시다](https://youtu.be/1Q_etC_GHHk?si=IIIlVkXuR6s-_yF0)

<br>

> 작성 - `2025.09.16`<br>
> 마지막 수정 - `2025.09.16`