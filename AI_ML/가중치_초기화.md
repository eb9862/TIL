# 가중치 초기화 (Weight Initialization)

## 개요

가중치 초기화는 신경망 학습의 첫 단계로, 각 층의 가중치를 적절히 설정하는 과정입니다

적절한 초기화는 학습 속도와 안정성에 큰 영향을 미치며, 잘못된 초기화는 기울기 소실(vanishing gradient)이나 기울기 폭주(exploding gradient)를 초래할 수 있습니다

- **너무 큰 초기값**: 층을 통과할수록 출력값이 급격히 커져, Sigmoid/Tanh는 포화 영역에 들어가 기울기가 거의 0이 되거나, ReLU 계열은 폭주 가능

- **너무 작은 초기값**: 출력값이 거의 0에 수렴하여 학습 속도가 매우 느려짐

따라서 초기 가중치는 **적절한 분산을 가지는 랜덤 값**으로 설정하는 것이 중요합니다

## 주요 초기화 기법

### 1. Xavier 초기화 (Glorot Initialization)

#### 설명
- 입력과 출력 뉴런 수를 고려하여 가중치를 초기화합니다. 주로 Sigmoid, Tanh 활성화 함수에 적합합니다.

- 층 출력의 분산을 일정하게 유지하여 학습 초기에 기울기 소실 문제를 완화합니다.

#### 수식

- **균등 분포**
    
  ⇒ $W \sim \mathcal{U}\left(-\sqrt{\frac{6}{n_{\text{in}} + n_{\text{out}}}}, \sqrt{\frac{6}{n_{\text{in}} + n_{\text{out}}}}\right)$

- **정규 분포**
    
    ⇒ $W \sim \mathcal{N}\left(0, \frac{2}{n_{\text{in}} + n_{\text{out}}}\right)$

- $n_{\text{in}}$: 입력 뉴런 수  
- $n_{\text{out}}$: 출력 뉴런 수  

> 입력과 출력 뉴런 수를 고려하여 분산을 조정하면, 층을 통과할 때 출력 분산이 일정하게 유지됩니다

#### 장점
  - 출력값 분산 일정 → 기울기 소실/폭주 완화
  - Sigmoid, Tanh 활성화 함수에 적합
#### 단점
  - ReLU 계열에서는 활성화 값이 0으로 치우칠 수 있어 부적합

### 2. He 초기화 (Kaiming Initialization)

#### 설명

- ReLU 계열 활성화 함수에 최적화된 초기화 방법입니다

- ReLU는 음수 입력에 대해 출력을 0으로 만들기 때문에, 초기 가중치 분산을 더 크게 설정하여 기울기 소실을 방지합니다

#### 수식

⇒ $W \sim \mathcal{N}\left(0, \frac{2}{n_{\text{in}}}\right)$

#### 장점
- ReLU 특성을 고려 → 학습 안정성과 속도 향상
- 깊은 네트워크에서도 기울기 소실 문제 완화

#### 단점

- Sigmoid, Tanh에는 부적합

## 결론

- 초기 가중치는 단순 랜덤 값보다 **활성화 함수와 층 구조를 고려한 초기화**가 중요합니다 

- Xavier 초기화와 He 초기화를 적절히 사용하면 학습 안정성과 속도를 크게 향상시킬 수 있으며, 기울기 소실/폭주 문제를 효과적으로 완화할 수 있습니다

- 특히, 부적절한 초기화는 학습 속도를 늦추거나 학습 자체가 불가능하게 만들 수 있으므로, 모델 특성에 맞는 초기화 선택이 필수적입니다

<br>

> 작성 - `2025.09.16`<br>
> 마지막 수정 - `2025.09.16`

